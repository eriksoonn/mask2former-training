{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17a42d35-458d-4a0d-b813-23a65f6bb0c4",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3d9da-d6b8-4f88-9c4b-b1d3d7c55215",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9ca1d-f083-4c94-a290-4b42a71d8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4063413-be3d-49fc-abcb-8a31e86c18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageSegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(images_dir) if not f.startswith('.')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.filenames[idx] + '.jpg')\n",
    "        mask_path = os.path.join(self.masks_dir, self.filenames[idx] + '.png') \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\") \n",
    "        batch={\"pixel_values\": image, \n",
    "               \"labels\": mask\n",
    "              }\n",
    "        if self.transform:\n",
    "            batch = self.transform(batch)\n",
    "            for k,v in batch.items():\n",
    "                batch[k].squeeze_() # remove batch dimension\n",
    "                v[v==255]=1\n",
    "                \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328da139-b53a-4dab-a53b-f018a4dabbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"/home/erik/Documents/Finetune-Mask2Former/data/rs19/images/train\"\n",
    "masks_dir = \"/home/erik/Documents/Finetune-Mask2Former/data/rs19/labels/train\"\n",
    "train_ds = ImageSegmentationDataset(images_dir, masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f8197-21a5-488e-8d64-b21c2f4cfd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=train_ds[5]['pixel_values']\n",
    "label=train_ds[5]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f41885-a345-41e1-9f4a-e9c9890b1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the original and transformed images\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Image')\n",
    "plt.imshow(image)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Label')\n",
    "plt.imshow(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92ab8b-94ec-43fe-9817-75f6d846b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_image = np.array(image)\n",
    "np_label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161df3cd-7bc5-4dcd-8246-78a78063e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation\n",
    "HFlip = A.Compose([\n",
    "    A.HorizontalFlip(p=1),  \n",
    "])\n",
    "# Define the transformation\n",
    "VFlip = A.Compose([\n",
    "    A.VerticalFlip(p=1),  \n",
    "])\n",
    "CJ = A.Compose([\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.7, hue=0.2, p=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c18047-8805-42e5-a4f7-a6654c3321fe",
   "metadata": {},
   "source": [
    "## Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480d998-2549-4770-948d-6998a88515c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply the transformation\n",
    "transformed = HFlip(image=np_image)\n",
    "transformed_image = transformed[\"image\"]\n",
    "\n",
    "# Display the original and transformed images\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(np_image)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Transformed Image')\n",
    "plt.imshow(transformed_image)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631159c3-8306-4145-a73d-5b28be557df2",
   "metadata": {},
   "source": [
    "## Vertical Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fc158-a1cb-4d0b-96b2-9231b6e4b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply the transformation\n",
    "transformed = VFlip(image=np_image)\n",
    "transformed_image = transformed[\"image\"]\n",
    "\n",
    "# Display the original and transformed images\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(np_image)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Transformed Image')\n",
    "plt.imshow(transformed_image)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001572c9-6b62-406e-958e-6b152f9641e4",
   "metadata": {},
   "source": [
    "## ColorJitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd0470-110d-43bd-a54d-04129ce28b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply the transformation\n",
    "transformed = CJ(image=np_image)\n",
    "transformed_image = transformed[\"image\"]\n",
    "\n",
    "# Display the original and transformed images\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(np_image)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Transformed Image')\n",
    "plt.imshow(transformed_image)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f90958-03ce-4307-baf2-2f4238d8868d",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b0b49-5c3c-434c-a3db-93bc2ae00379",
   "metadata": {},
   "source": [
    "### Create dataset dict with the images and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419dea99-4790-43e8-ac76-bc958e1b9c56",
   "metadata": {},
   "source": [
    "### The Dataset dict is created by passing in the image directory as well as the labels directory. Images are in jpg format while the labels are in png format. The dictionary entries can be accessed by indexing e.g. dataset[idx]. The images and labels are converted to arrays. The label's pixel_values are also converted from 255 to 1 in order to match the output from argmax from the model's prediction later. For each dictionary entries, there are two elements.\n",
    "### To access image, dataset[idx][0]. \n",
    "### To access label, dataset[idx][1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5852a-bcef-484a-bb93-7ca37ac4658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader \n",
    "import torch\n",
    "\n",
    "class ImageSegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(images_dir) if not f.startswith('.')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.filenames[idx] + '.jpg')\n",
    "        mask_path = os.path.join(self.masks_dir, self.filenames[idx] + '.png') \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        np_image=np.array(image)\n",
    "        # convert to C, H, W\n",
    "        np_image = np_image.transpose(2,0,1)\n",
    "        mask = Image.open(mask_path) \n",
    "        np_mask=np.array(mask)\n",
    "        np_mask[np_mask==255]=1\n",
    "                \n",
    "        return np_image, np_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afc879-b261-48a2-95f7-e38aadbd5d84",
   "metadata": {},
   "source": [
    "# Defining Data Module from pytorch lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967fa4e7-d3ff-43e2-b26a-658f6bcfafc6",
   "metadata": {},
   "source": [
    "### Defining Data Module as Mask2Former takes in class_labels and mask_labels as a list of tensors and pytorch lightning is unable to load it directly as it expects tensor instead of a list from dataloader.\n",
    "### collate_fn is used to group images and labels together for each batch instead of each batch containing an image and label.\n",
    "### While loading the dataloaders, Mask2FormerImageProcessor  is used to process the images and labels for each batch. Mask2FormerImageProcessor normalizes the input using ImageNet mean = (0.485, 0.456, 0.406) and std = (0.229, 0.224, 0.225) and then convert it to tensor. \n",
    "### The output size from the Mask2FormerImageProcessor is a batch of images with size (2, 3, 640, 640), a batch of pixel_masks of size (2, 640, 640), which is used to signify the images is not masked, a mask_labels of size (2,640,640) and a class_labels of size (2). The label is converted to lists of binary masks and their respective labels which are mask_labels and class_labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b9be7-5646-4e9d-8f2f-d74092c4dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    images=inputs[0]\n",
    "    segmentation_maps=inputs[1]\n",
    "    batch = processor(\n",
    "        images,\n",
    "        segmentation_maps=segmentation_maps,\n",
    "        size=(640,640),\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    batch[\"original_images\"] = images\n",
    "    batch[\"original_segmentation_maps\"] = segmentation_maps\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a00ff39-5e6a-4cc4-a1ee-53f794b7521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dataset_dir, batch_size, num_workers, processor=None):\n",
    "        super().__init__()\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.processor = processor\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = ImageSegmentationDataset(images_dir=os.path.join(self.dataset_dir, 'images', 'train'),\n",
    "                                                          masks_dir=os.path.join(self.dataset_dir, 'labels', 'train'),\n",
    "                                                          transform=None) # Add your transforms here\n",
    "            self.val_dataset = ImageSegmentationDataset(images_dir=os.path.join(self.dataset_dir, 'images', 'val'),\n",
    "                                                        masks_dir=os.path.join(self.dataset_dir, 'labels', 'val'),\n",
    "                                                        transform=None) # Add your transforms here\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = ImageSegmentationDataset(images_dir=os.path.join(self.dataset_dir, 'images', 'test'),\n",
    "                                                         masks_dir=os.path.join(self.dataset_dir, 'labels', 'test'),\n",
    "                                                         transform=None) # Add your transforms here\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, collate_fn=collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=collate_fn)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c13a5a9-b7db-48d1-9982-2f76ccfc377b",
   "metadata": {},
   "source": [
    "### Initialise Mask2Former processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2f6bd-7014-4479-b498-7677ccedc5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-ade-semantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5ed95b2-9220-43ec-af6a-2ef1cb7c946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "num_workers=4\n",
    "\n",
    "data_module = SegmentationDataModule(dataset_dir='/home/erik/Documents/Finetune-Mask2Former/data/rs19', batch_size=batch_size, num_workers=num_workers, processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7a9406-5815-41a4-9efa-7cbad6522820",
   "metadata": {},
   "source": [
    "### Check dataloader has the proper config for Mask2Former model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6911a0d-11ea-4ab0-82ef-c1d96904e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup(stage='fit')\n",
    "train_dataloader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27e56d85-37f4-4261-bb53-2d80f6911a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([1, 3, 640, 640])\n",
      "pixel_mask torch.Size([1, 640, 640])\n",
      "mask_labels torch.Size([13, 640, 640])\n",
      "class_labels torch.Size([13])\n",
      "original_images (3, 1080, 1920)\n",
      "original_segmentation_maps (1080, 1920)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k,v.shape)\n",
    "  else:\n",
    "    print(k,v[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949d7cf-eb5d-4117-a3b1-8683f752ce49",
   "metadata": {},
   "source": [
    "### Creating id for the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4500c02e-3896-4616-a226-e572e7a03c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Background',\n",
       " 1: 'road',\n",
       " 2: 'sidewalk',\n",
       " 3: 'construction',\n",
       " 4: 'tram-track',\n",
       " 5: 'fence',\n",
       " 6: 'pole',\n",
       " 7: 'traffic-light',\n",
       " 8: 'traffic-sign',\n",
       " 9: 'vegetation',\n",
       " 10: 'terrain',\n",
       " 11: 'sky',\n",
       " 12: 'human',\n",
       " 13: 'rail-track',\n",
       " 14: 'car',\n",
       " 15: 'truck',\n",
       " 16: 'trackbed',\n",
       " 17: 'on-rails',\n",
       " 18: 'rail-raised',\n",
       " 19: 'rail-embedded'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newid2label = {\n",
    "    0: \"Background\",\n",
    "    1: \"road\",\n",
    "    2: \"sidewalk\",\n",
    "    3: \"construction\",\n",
    "    4: \"tram-track\",\n",
    "    5: \"fence\",\n",
    "    6: \"pole\",\n",
    "    7: \"traffic-light\",\n",
    "    8: \"traffic-sign\",\n",
    "    9: \"vegetation\",\n",
    "    10: \"terrain\",\n",
    "    11: \"sky\",\n",
    "    12: \"human\",\n",
    "    13: \"rail-track\",\n",
    "    14: \"car\",\n",
    "    15: \"truck\",\n",
    "    16: \"trackbed\",\n",
    "    17: \"on-rails\",\n",
    "    18: \"rail-raised\",\n",
    "    19: \"rail-embedded\"\n",
    "}\n",
    "newid2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2717ad9-7a0b-4749-ac7e-ec48a010b071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Background': 0,\n",
       " 'road': 1,\n",
       " 'sidewalk': 2,\n",
       " 'construction': 3,\n",
       " 'tram-track': 4,\n",
       " 'fence': 5,\n",
       " 'pole': 6,\n",
       " 'traffic-light': 7,\n",
       " 'traffic-sign': 8,\n",
       " 'vegetation': 9,\n",
       " 'terrain': 10,\n",
       " 'sky': 11,\n",
       " 'human': 12,\n",
       " 'rail-track': 13,\n",
       " 'car': 14,\n",
       " 'truck': 15,\n",
       " 'trackbed': 16,\n",
       " 'on-rails': 17,\n",
       " 'rail-raised': 18,\n",
       " 'rail-embedded': 19}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlabel2id = {v: k for k, v in newid2label.items()}\n",
    "newlabel2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc2a95da-9fa4-466b-8652-44de14212bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Mask2FormerForUniversalSegmentation\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric\n",
    "#from datasets import load_metric\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d30e17-fe84-4f56-86cd-6c8f57b328ae",
   "metadata": {},
   "source": [
    "## Defining Pytorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a11ce9c-ec36-4149-89d0-81c06896f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask2FormerFinetuner(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, id2label):\n",
    "        super(Mask2FormerFinetuner, self).__init__()\n",
    "        self.id2label = id2label\n",
    "        self.num_classes = len(id2label.keys())\n",
    "        self.label2id = {v:k for k,v in self.id2label.items()}\n",
    "        self.model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "            \"facebook/mask2former-swin-small-ade-semantic\",\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "        evaluate.load\n",
    "        self.train_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.val_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.test_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        \n",
    "    def forward(self, pixel_values, mask_labels=None, class_labels=None):\n",
    "        # Your model's forward method\n",
    "        return self.model(pixel_values=pixel_values, mask_labels=mask_labels, class_labels=class_labels)\n",
    "        \n",
    "    def transfer_batch_to_device(self, batch, device, dataloader_idx=0):\n",
    "        batch['pixel_values'] = batch['pixel_values'].to(device)\n",
    "        batch['mask_labels'] = [label.to(device) for label in batch['mask_labels']]\n",
    "        batch['class_labels'] = [label.to(device) for label in batch['class_labels']]\n",
    "        return batch\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            pixel_values=batch[\"pixel_values\"],\n",
    "            mask_labels=batch[\"mask_labels\"],\n",
    "            class_labels=batch[\"class_labels\"],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            pixel_values=batch[\"pixel_values\"],\n",
    "            mask_labels=[labels for labels in batch[\"mask_labels\"]],\n",
    "            class_labels=[labels for labels in batch[\"class_labels\"]],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            pixel_values=batch[\"pixel_values\"],\n",
    "            mask_labels=[labels for labels in batch[\"mask_labels\"]],\n",
    "            class_labels=[labels for labels in batch[\"class_labels\"]],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        original_images = batch[\"original_images\"]\n",
    "        ground_truth = batch[\"original_segmentation_maps\"]\n",
    "        target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "        # predict segmentation maps\n",
    "        predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,target_sizes=target_sizes)\n",
    "        # Optionally log loss here\n",
    "        metrics = self.train_mean_iou._compute(\n",
    "            predictions=predicted_segmentation_maps[0].cpu().numpy(),\n",
    "            references=ground_truth[0],\n",
    "            num_labels=self.num_classes,\n",
    "            ignore_index=254,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        # Extract per category metrics and convert to list if necessary (pop before defining the metrics dictionary)\n",
    "        per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "        per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "    \n",
    "        # Re-define metrics dict to include per-category metrics directly\n",
    "        metrics = {\n",
    "            'loss': loss, \n",
    "            \"mean_iou\": metrics[\"mean_iou\"], \n",
    "            \"mean_accuracy\": metrics[\"mean_accuracy\"],\n",
    "            **{f\"accuracy_{self.id2label[i]}\": v for i, v in enumerate(per_category_accuracy)},\n",
    "            **{f\"iou_{self.id2label[i]}\": v for i, v in enumerate(per_category_iou)}\n",
    "        }\n",
    "        for k,v in metrics.items():\n",
    "            self.log(k,v,sync_dist=True,batch_size=batch_size)\n",
    "        return(metrics)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba56850-a250-4047-b4bc-70b76113c005",
   "metadata": {},
   "source": [
    "# Initialising Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc93cf-3fbf-4c8f-ba12-72c029248d3b",
   "metadata": {},
   "source": [
    "### Load previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7158e705-8bcd-49e8-8482-3b980fc11c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-small-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([21]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([21, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([21]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Mask2FormerFinetuner:\n\tsize mismatch for model.model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight: copying a param with shape torch.Size([128, 3, 4, 4]) from checkpoint, the shape in current model is torch.Size([96, 3, 4, 4]).\n\tsize mismatch for model.model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.embeddings.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.embeddings.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 4]) from checkpoint, the shape in current model is torch.Size([169, 3]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([384, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([96, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 4]) from checkpoint, the shape in current model is torch.Size([169, 3]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([384, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([96, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([192, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 8]) from checkpoint, the shape in current model is torch.Size([169, 6]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 8]) from checkpoint, the shape in current model is torch.Size([169, 6]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight: copying a param with shape torch.Size([1024, 2048]) from checkpoint, the shape in current model is torch.Size([768, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 32]) from checkpoint, the shape in current model is torch.Size([169, 24]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight: copying a param with shape torch.Size([1024, 4096]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 32]) from checkpoint, the shape in current model is torch.Size([169, 24]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight: copying a param with shape torch.Size([1024, 4096]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage3.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage4.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage4.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.decoder.input_projections.0.0.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 768, 1, 1]).\n\tsize mismatch for model.model.pixel_level_module.decoder.input_projections.1.0.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 384, 1, 1]).\n\tsize mismatch for model.model.pixel_level_module.decoder.input_projections.2.0.weight: copying a param with shape torch.Size([256, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 192, 1, 1]).\n\tsize mismatch for model.model.pixel_level_module.decoder.adapter_1.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 96, 1, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/erik/Documents/Finetune-Mask2Former/outputs/lightning_logs_csv/version_4/checkpoints/epoch=6-step=6699.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMask2FormerFinetuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mid2label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewid2label\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Mask2Former/lib/python3.10/site-packages/pytorch_lightning/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Mask2Former/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1662\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \n\u001b[1;32m   1661\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1662\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/miniconda3/envs/Mask2Former/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m~/miniconda3/envs/Mask2Former/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:187\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    184\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keys\u001b[38;5;241m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m~/miniconda3/envs/Mask2Former/lib/python3.10/site-packages/torch/nn/modules/module.py:2624\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2616\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2617\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2619\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2620\u001b[0m             ),\n\u001b[1;32m   2621\u001b[0m         )\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2626\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2627\u001b[0m         )\n\u001b[1;32m   2628\u001b[0m     )\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Mask2FormerFinetuner:\n\tsize mismatch for model.model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight: copying a param with shape torch.Size([128, 3, 4, 4]) from checkpoint, the shape in current model is torch.Size([96, 3, 4, 4]).\n\tsize mismatch for model.model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.embeddings.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.embeddings.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 4]) from checkpoint, the shape in current model is torch.Size([169, 3]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([384, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([96, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 4]) from checkpoint, the shape in current model is torch.Size([169, 3]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([96, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([384, 96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([96, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([192, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 8]) from checkpoint, the shape in current model is torch.Size([169, 6]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 8]) from checkpoint, the shape in current model is torch.Size([169, 6]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([192, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([768, 192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([192, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 16]) from checkpoint, the shape in current model is torch.Size([169, 12]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight: copying a param with shape torch.Size([1024, 2048]) from checkpoint, the shape in current model is torch.Size([768, 1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 32]) from checkpoint, the shape in current model is torch.Size([169, 24]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight: copying a param with shape torch.Size([1024, 4096]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table: copying a param with shape torch.Size([529, 32]) from checkpoint, the shape in current model is torch.Size([169, 24]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_index: copying a param with shape torch.Size([144, 144]) from checkpoint, the shape in current model is torch.Size([49, 49]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight: copying a param with shape torch.Size([4096, 1024]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight: copying a param with shape torch.Size([1024, 4096]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for model.model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage3.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage4.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.encoder.hidden_states_norms.stage4.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.pixel_level_module.decoder.input_projections.0.0.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 768, 1, 1]).\n\tsize mismatch for model.model.pixel_level_module.decoder.input_projections.1.0.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 384, 1, 1]).\n\tsize mismatch for model.model.pixel_level_module.decoder.input_projections.2.0.weight: copying a param with shape torch.Size([256, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 192, 1, 1]).\n\tsize mismatch for model.model.pixel_level_module.decoder.adapter_1.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 96, 1, 1])."
     ]
    }
   ],
   "source": [
    "path=\"/home/erik/Documents/Finetune-Mask2Former/outputs/lightning_logs_csv/version_4/checkpoints/epoch=6-step=6699.ckpt\"\n",
    "model = Mask2FormerFinetuner.load_from_checkpoint(path,id2label=newid2label,ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d3ddc-d030-48d4-9304-9c44ee303aed",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a4cfe0-81a7-433c-aacc-f2cb780b1b2e",
   "metadata": {},
   "source": [
    "### Training, Val and test is done using python instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ea3d5-a261-4b77-990d-ea2e2558c43a",
   "metadata": {},
   "source": [
    "# Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f034736d-b901-4c4f-8c5f-c5c291dce68f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5755c95d-1b30-4d53-a38f-252cb854ac79",
   "metadata": {},
   "source": [
    "### Function to get the respective batch by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47c255-a85d-4817-a5f0-2371acf7a6c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90640d-4fd6-415b-bb3f-1ed3c41fb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup(stage='test')\n",
    "test_dataloader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff025e-50fb-437f-ad80-72c791d192a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312674a-e4c2-45be-9af5-3b3ec14e6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(index):\n",
    "    data_module.setup(stage='test')\n",
    "    test_dataloader = data_module.test_dataloader()\n",
    "    test_iterator = iter(test_dataloader)\n",
    "    for i in range(index):\n",
    "        batch = next(test_iterator)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6ddb1-320d-4ffe-89c7-44fc96159d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=getBatch(177)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c713e8-f2c4-4f1b-94d9-5324cb7735f4",
   "metadata": {},
   "source": [
    "### Function to view mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504e1ce-1043-4792-b6c6-e0f91b4c234d",
   "metadata": {},
   "source": [
    "### Function to do predictions for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b280cc6-5b50-448f-91bf-b3d4b3b337f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def viewMask(batch):\n",
    "    # Plot expects (height, width, channels), hence transpose.\n",
    "    image=batch[\"original_images\"][0]\n",
    "    label=batch[\"original_segmentation_maps\"][0]\n",
    "    image_transpose = np.transpose(image, (1, 2, 0))\n",
    "    \n",
    "    f, axs = plt.subplots(1, 2)\n",
    "    f.set_figheight(30)\n",
    "    f.set_figwidth(50)\n",
    "    axs[0].set_title(\"Image\", {'fontsize': 40})\n",
    "    axs[0].imshow(image_transpose)\n",
    "    axs[1].set_title(\"Ground truth\", {'fontsize': 40})\n",
    "    axs[1].imshow(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d429cf3-cf81-4d63-97cb-df46cf4ac5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewMask(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d9759-2d09-44fe-9899-0679ad8d2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size=1\n",
    "def batch_prediction(batch):\n",
    "    original_images = batch[\"original_images\"]\n",
    "    target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "    outputs = model(\n",
    "          pixel_values=batch[\"pixel_values\"].to(device),\n",
    "          mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "          class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "      )\n",
    "    result = processor.post_process_semantic_segmentation(outputs,target_sizes=target_sizes)[0].cpu().numpy()\n",
    "    original_segmentation_maps = batch[\"original_segmentation_maps\"][0]\n",
    "    return result, original_segmentation_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ef7b4-9530-4e83-9ce7-ecc7d69ce079",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_semantic_map,label=batch_prediction(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b50da-7a23-48e7-96cb-6c6e34af6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viewPrediction(prediction,label):\n",
    "    # Need to convert image and labels to numpy to plot. \n",
    "    f, axs = plt.subplots(1, 2)\n",
    "    f.set_figheight(30)\n",
    "    f.set_figwidth(50)\n",
    "    axs[0].set_title(\"Prediction\", {'fontsize': 40})\n",
    "    axs[0].imshow(prediction)\n",
    "    axs[1].set_title(\"Ground truth\", {'fontsize': 40})\n",
    "    axs[1].imshow(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091aed4-f80c-474d-a1c1-1d606c48456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewPrediction(pred_semantic_map,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41daca0a-c20f-4e26-b059-f14ca6f0ed67",
   "metadata": {},
   "source": [
    "### Define a function to display a prediction overlay in which blue represents correct prediction while red represent in correct prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326fadd8-d068-4abb-890a-3603ccc12a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrongPredictionOverlay(image,label):\n",
    "    matches = image == label\n",
    "    # Create an array highlighting wrong predictions: 0 for correct predictions, 2 for mismatches\n",
    "    wrong_predictions = np.where(matches, 0, image)\n",
    "    wrong_predictions[wrong_predictions==1]=2\n",
    "    array_3d_colored = np.zeros((image.shape[0], image.shape[1], 3), dtype=np.uint8)\n",
    "    color_map = {\n",
    "        0:(0,0,0),\n",
    "        1:(0,128,128),\n",
    "        2:(255,0,0)\n",
    "        }\n",
    "    for id, color in color_map.items():\n",
    "        array_3d_colored[image == id] = color\n",
    "        array_3d_colored[wrong_predictions == id] = color\n",
    "    \n",
    "    # Display the original and transformed images\n",
    "    f, axs = plt.subplots(1, 2)\n",
    "    f.set_figheight(30)\n",
    "    f.set_figwidth(50)\n",
    "    axs[0].set_title(\"Prediction overlay\", {'fontsize': 40})\n",
    "    axs[0].imshow(array_3d_colored)\n",
    "    axs[1].set_title(\"Ground truth\", {'fontsize': 40})\n",
    "    axs[1].imshow(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f2234-e807-4288-a8a0-22c0b94e8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongPredictionOverlay(pred_semantic_map,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3d6dd-4b9f-4f9a-9a0e-be08be6e75d4",
   "metadata": {},
   "source": [
    "### Function to compute Miou for single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d67837-85ec-454a-8760-ea737aef422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miou(pred,label):\n",
    "    metric = evaluate.load(\"mean_iou\")\n",
    "    # Now your data should match the expected format:\n",
    "    metrics = metric._compute(\n",
    "        predictions=pred,\n",
    "        references=label,\n",
    "        num_labels=len(newid2label),\n",
    "        ignore_index=254\n",
    "    )\n",
    "      # add per category metrics as individual key-value pairs\n",
    "    # Extract per category metrics and convert to list if necessary (pop before defining the metrics dictionary)\n",
    "    per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "    per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "\n",
    "    # Re-define metrics dict to include per-category metrics directly\n",
    "    metrics = {\n",
    "        \"mean_iou\": metrics[\"mean_iou\"], \n",
    "        \"mean_accuracy\": metrics[\"mean_accuracy\"],\n",
    "        **{f\"accuracy_{newid2label[i]}\": v for i, v in enumerate(per_category_accuracy)},\n",
    "        **{f\"iou_{newid2label[i]}\": v for i, v in enumerate(per_category_iou)}\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2657e-50a7-40e1-a70e-299702eaebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "miou(pred_semantic_map,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2584c66-e9df-46af-94a5-b07236d55fc2",
   "metadata": {},
   "source": [
    "### Function to sort all the predictions of the dataset into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889087db-ab7e-4411-b747-70fb39f6021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_prediction(test_dataloader):\n",
    "    result_list=[]\n",
    "    original_seg_maps_list=[]\n",
    "    for batch in test_dataloader:\n",
    "        original_images = batch[\"original_images\"]\n",
    "        target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "        outputs = model(\n",
    "              pixel_values=batch[\"pixel_values\"].to(device),\n",
    "              mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "              class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "          )\n",
    "        result = processor.post_process_semantic_segmentation(outputs,target_sizes=target_sizes)[0]\n",
    "        result=result.cpu().numpy()\n",
    "        result_list.append(result)\n",
    "        original_seg_maps_list.append(batch['original_segmentation_maps'][0])\n",
    "    return result_list, original_seg_maps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54659a87-e031-4015-99c5-70fb571baf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list, original_seg_maps_list=dataset_prediction(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b962c2d-1f65-455a-8465-3c5c30464088",
   "metadata": {},
   "source": [
    "### Function to compute mean-Iou for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80800c-0552-4a16-a791-bb9037843b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateset_miou(result_list, original_seg_maps_list):\n",
    "    metric = evaluate.load(\"mean_iou\")\n",
    "    # Now your data should match the expected format:\n",
    "    metrics = metric._compute(\n",
    "        predictions=result_list,\n",
    "        references=original_seg_maps_list,\n",
    "        num_labels=len(newid2label),\n",
    "        ignore_index=254\n",
    "    )\n",
    "      # add per category metrics as individual key-value pairs\n",
    "    # Extract per category metrics and convert to list if necessary (pop before defining the metrics dictionary)\n",
    "    per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "    per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "    \n",
    "    # Re-define metrics dict to include per-category metrics directly\n",
    "    metrics = {\n",
    "        \"mean_iou\": metrics[\"mean_iou\"], \n",
    "        \"mean_accuracy\": metrics[\"mean_accuracy\"],\n",
    "        **{f\"accuracy_{newid2label[i]}\": v for i, v in enumerate(per_category_accuracy)},\n",
    "        **{f\"iou_{newid2label[i]}\": v for i, v in enumerate(per_category_iou)}\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb9b0d-b63d-4585-8e96-a1205401c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateset_miou(result_list, original_seg_maps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc61481-47d8-4a43-8011-00be02b17377",
   "metadata": {},
   "outputs": [],
   "source": [
    "    f, axs = plt.subplots(1, 2)\n",
    "    f.set_figheight(30)\n",
    "    f.set_figwidth(50)\n",
    "    axs[0].set_title(\"Image\", {'fontsize': 40})\n",
    "    axs[0].imshow(image_transpose)\n",
    "    axs[1].set_title(\"Ground truth\", {'fontsize': 40})\n",
    "    axs[1].imshow(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b04f4-1dfc-43ee-b183-b49a2c4a75b3",
   "metadata": {},
   "source": [
    "### Function to save all the predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68111496-019b-439c-8ad3-8977cb41195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def savePredictions(result_list, original_seg_maps_list, save_path):\n",
    "    for i in range(len(result_list)):\n",
    "        file_name = f\"result_{i}\"\n",
    "        # Set up the plot\n",
    "        f, axs = plt.subplots(1, 2)\n",
    "        f.set_figheight(30)\n",
    "        f.set_figwidth(50)\n",
    "        \n",
    "        axs[0].set_title(\"Prediction\", {'fontsize': 40})\n",
    "        axs[0].imshow(result_list[i])\n",
    "        axs[1].set_title(\"Ground truth\", {'fontsize': 40})\n",
    "        axs[1].imshow(original_seg_maps_list[i])\n",
    "    \n",
    "        # Construct the full path where the image will be saved\n",
    "        file_path = os.path.join(save_path, f\"{file_name}.png\")\n",
    "    \n",
    "        # Save the figure\n",
    "        plt.savefig(file_path, bbox_inches='tight')\n",
    "        plt.close(f)  # Close the figure to free memory\n",
    "    print(\"Predictions saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b1b89-fe44-4773-8dd1-07898dca840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path='data_augmentation_dataset/outputs-mask2former/'\n",
    "savePredictions(result_list, original_seg_maps_list,save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mask2Former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
