{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fdda6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standalone cell: ORIGINAL | GT (colored) | OVERLAY as three separate images ---\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Import your project pieces\n",
    "from mask2former import (\n",
    "    SegmentationDataModule, \n",
    "    DATASET_DIR, \n",
    "    BATCH_SIZE, \n",
    "    NUM_WORKERS, \n",
    "    ID2LABEL,\n",
    ")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "\n",
    "def _to_numpy_image(img_t):\n",
    "    \"\"\"\n",
    "    Accepts CHW or HWC torch/numpy; returns uint8 HWC numpy.\n",
    "    Handles [0,1] floats or [0,255] uint8.\n",
    "    \"\"\"\n",
    "    if isinstance(img_t, torch.Tensor):\n",
    "        img = img_t.detach().cpu().numpy()\n",
    "    else:\n",
    "        img = np.asarray(img_t)\n",
    "\n",
    "    # CHW -> HWC if needed\n",
    "    if img.ndim == 3 and img.shape[0] in (1, 3):\n",
    "        img = np.moveaxis(img, 0, -1)\n",
    "\n",
    "    # If single channel, stack to 3 channels for nicer visualization\n",
    "    if img.ndim == 2:\n",
    "        img = np.stack([img]*3, axis=-1)\n",
    "\n",
    "    # normalize to 0..255 uint8\n",
    "    img_float = img.astype(np.float32)\n",
    "    if img_float.max() <= 1.0:\n",
    "        img_float = img_float * 255.0\n",
    "    img_uint8 = np.clip(img_float, 0, 255).astype(np.uint8)\n",
    "    return img_uint8\n",
    "\n",
    "def _make_palette(id2label, seed=13):\n",
    "    \"\"\"\n",
    "    Deterministic color palette for classes: returns ListedColormap + id->color map.\n",
    "    Background (0) set to black for contrast (if present).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    max_id = max(id2label.keys()) if len(id2label) else 0\n",
    "    num_classes = max_id + 1\n",
    "    colors = rng.uniform(0, 1, size=(num_classes, 3))\n",
    "    if 0 in id2label:\n",
    "        colors[0] = np.array([0.0, 0.0, 0.0])\n",
    "    cmap = ListedColormap(colors)\n",
    "    id2color = {i: colors[i] for i in range(num_classes)}\n",
    "    return cmap, id2color\n",
    "\n",
    "def _legend_handles(mask, id2label, id2color):\n",
    "    present_ids = np.unique(mask)\n",
    "    handles = []\n",
    "    for i in present_ids:\n",
    "        i = int(i)\n",
    "        label = id2label.get(i, f\"id_{i}\")\n",
    "        col = id2color.get(i, (0.5, 0.5, 0.5))\n",
    "        handles.append(Patch(facecolor=col, edgecolor='none', label=f\"{i}: {label}\"))\n",
    "    if not handles:\n",
    "        handles = [Patch(facecolor='gray', edgecolor='none', label=\"(no labels)\")]\n",
    "    return handles\n",
    "\n",
    "def visualize_image_gt_overlay(batch, id2label, sample_index=0, overlay_alpha=0.5, figsize=(18,6)):\n",
    "    \"\"\"\n",
    "    Shows three separate panels:\n",
    "      1) Original image\n",
    "      2) Ground-truth (colored) + legend\n",
    "      3) Overlay: original image with GT mask blended on top\n",
    "    Expects batch keys: \"original_images\", \"original_segmentation_maps\"\n",
    "    \"\"\"\n",
    "    orig_imgs = batch[\"original_images\"]\n",
    "    gts = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "    img = _to_numpy_image(orig_imgs[sample_index])\n",
    "    gt = gts[sample_index]\n",
    "    if isinstance(gt, torch.Tensor):\n",
    "        gt = gt.detach().cpu().numpy()\n",
    "    else:\n",
    "        gt = np.asarray(gt)\n",
    "\n",
    "    cmap, id2color = _make_palette(id2label)\n",
    "    vmax = max(id2label.keys()) if len(id2label) else 0\n",
    "\n",
    "    # Plot as three separate subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "    # 1) Original\n",
    "    ax = axes[0]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(\"Original\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # 2) GT (colored)\n",
    "    ax = axes[1]\n",
    "    ax.imshow(gt, cmap=cmap, vmin=0, vmax=vmax, interpolation='nearest')\n",
    "    ax.set_title(\"Ground Truth\")\n",
    "    ax.axis(\"off\")\n",
    "    handles = _legend_handles(gt, id2label, id2color)\n",
    "    ax.legend(handles=handles, loc=\"lower right\", fontsize=8, frameon=True, ncol=1)\n",
    "\n",
    "    # 3) Overlay (image + GT)\n",
    "    ax = axes[2]\n",
    "    ax.imshow(img)\n",
    "    ax.imshow(gt, cmap=cmap, vmin=0, vmax=vmax, alpha=overlay_alpha, interpolation='nearest')\n",
    "    ax.set_title(\"Overlay (Image + GT)\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------- build a batch (standalone) ----------\n",
    "\n",
    "# Prefer test if available, else val, else train\n",
    "dm = SegmentationDataModule(dataset_dir=DATASET_DIR, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "dl = None\n",
    "try:\n",
    "    dm.setup(\"test\")\n",
    "    dl = dm.test_dataloader()\n",
    "except Exception:\n",
    "    dl = None\n",
    "\n",
    "if dl is None:\n",
    "    try:\n",
    "        dm.setup(\"validate\")\n",
    "        dl = dm.val_dataloader()\n",
    "    except Exception:\n",
    "        dl = None\n",
    "\n",
    "if dl is None:\n",
    "    dm.setup(\"fit\")\n",
    "    dl = dm.train_dataloader()\n",
    "\n",
    "batch = next(iter(dl))\n",
    "\n",
    "# ---------- visualize ----------\n",
    "visualize_image_gt_overlay(batch, ID2LABEL, sample_index=0, overlay_alpha=0.5, figsize=(18,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ebd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standalone cell: PROGRAMMABLE N SAMPLES (rows) -> [OG | OG+GT | OG+PRED] ---\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Project imports (adapt to your package layout if needed)\n",
    "from mask2former import (\n",
    "    Mask2FormerFinetuner,\n",
    "    SegmentationDataModule,\n",
    "    DATASET_DIR,\n",
    "    BATCH_SIZE,\n",
    "    NUM_WORKERS,\n",
    "    ID2LABEL,\n",
    "    LEARNING_RATE,\n",
    ")\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "CKPT_PATH = \"/home/erik/Documents/Finetune-Mask2Former/outputs/lightning_logs_csv/version_4/checkpoints/epoch=6-step=6699.ckpt\"   # change if your checkpoint is elsewhere\n",
    "N_SAMPLES  = 50                  # <--- number of rows to show\n",
    "OVERLAY_ALPHA = 0.5\n",
    "FIG_W = 18\n",
    "ROW_H = 3.0                      # figure height per row; total height = ROW_H * N_SAMPLES\n",
    "SEED_PALETTE = 13\n",
    "# ------------------------------------------\n",
    "\n",
    "def _to_numpy_image(img_t):\n",
    "    \"\"\"Accepts CHW or HWC torch/numpy; returns uint8 HWC numpy. Handles [0,1] floats.\"\"\"\n",
    "    if isinstance(img_t, torch.Tensor):\n",
    "        img = img_t.detach().cpu().numpy()\n",
    "    else:\n",
    "        img = np.asarray(img_t)\n",
    "\n",
    "    if img.ndim == 3 and img.shape[0] in (1, 3):  # CHW -> HWC\n",
    "        img = np.moveaxis(img, 0, -1)\n",
    "    if img.ndim == 2:                             # grayscale -> 3ch\n",
    "        img = np.stack([img]*3, axis=-1)\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "    if img.max() <= 1.0:\n",
    "        img = img * 255.0\n",
    "    return np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "def _make_palette(id2label, seed=SEED_PALETTE):\n",
    "    \"\"\"Deterministic palette for 0..max_id.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    max_id = max(id2label.keys()) if len(id2label) else 0\n",
    "    num_classes = max_id + 1\n",
    "    colors = rng.uniform(0, 1, size=(num_classes, 3))\n",
    "    if 0 in id2label:\n",
    "        colors[0] = np.array([0.0, 0.0, 0.0])\n",
    "    return ListedColormap(colors), max_id\n",
    "\n",
    "# --------- load data & model ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dm = SegmentationDataModule(dataset_dir=DATASET_DIR, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "# Prefer test -> val -> train\n",
    "dl = None\n",
    "try:\n",
    "    dm.setup(\"test\")\n",
    "    dl = dm.test_dataloader()\n",
    "    split_name = \"test\"\n",
    "except Exception:\n",
    "    dl = None\n",
    "\n",
    "if dl is None:\n",
    "    try:\n",
    "        dm.setup(\"validate\")\n",
    "        dl = dm.val_dataloader()\n",
    "        split_name = \"val\"\n",
    "    except Exception:\n",
    "        dl = None\n",
    "\n",
    "\n",
    "if dl is None:\n",
    "    raise RuntimeError(\"No test or val dataloader available â€” cannot visualize samples.\")\n",
    "\n",
    "print(f\"Showing samples from the **{split_name} split**\")\n",
    "\n",
    "model = Mask2FormerFinetuner.load_from_checkpoint(\n",
    "    CKPT_PATH,\n",
    "    id2label=ID2LABEL,\n",
    "    lr=LEARNING_RATE,\n",
    ").eval().to(device)\n",
    "\n",
    "# --------- collect up to N_SAMPLES ----------\n",
    "samples = []  # list of dicts: {img, gt, pred}\n",
    "cmap, vmax = _make_palette(ID2LABEL)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dl:\n",
    "        # batch keys expected: original_images, original_segmentation_maps, pixel_values\n",
    "        orig_imgs = batch[\"original_images\"]\n",
    "        gts       = batch[\"original_segmentation_maps\"]\n",
    "        pvals     = batch[\"pixel_values\"].to(device)\n",
    "\n",
    "        # forward entire batch at once\n",
    "        outputs = model.model(pixel_values=pvals)\n",
    "\n",
    "        # resize predictions to each original image size\n",
    "        target_sizes = []\n",
    "        for img in orig_imgs:\n",
    "            # support torch or numpy shapes [C,H,W] or [H,W,C]\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                arr = img.detach().cpu().numpy()\n",
    "            else:\n",
    "                arr = np.asarray(img)\n",
    "            if arr.ndim == 3 and arr.shape[0] in (1,3):  # CHW\n",
    "                H, W = arr.shape[1], arr.shape[2]\n",
    "            else:                                        # HWC\n",
    "                H, W = arr.shape[0], arr.shape[1]\n",
    "            target_sizes.append((H, W))\n",
    "\n",
    "        pred_list = model.processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n",
    "\n",
    "        # stash results\n",
    "        for i in range(len(orig_imgs)):\n",
    "            img_np = _to_numpy_image(orig_imgs[i])\n",
    "            gt_np  = gts[i].detach().cpu().numpy() if isinstance(gts[i], torch.Tensor) else np.asarray(gts[i])\n",
    "            pred_np = pred_list[i].detach().cpu().numpy()\n",
    "            samples.append({\"img\": img_np, \"gt\": gt_np, \"pred\": pred_np})\n",
    "            if len(samples) >= N_SAMPLES:\n",
    "                break\n",
    "        if len(samples) >= N_SAMPLES:\n",
    "            break\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples found. Check your dataloaders / dataset paths.\")\n",
    "\n",
    "# --------- plot grid: N_SAMPLES rows x 3 cols ----------\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(samples),\n",
    "    ncols=3,\n",
    "    figsize=(FIG_W, ROW_H * len(samples)),\n",
    ")\n",
    "\n",
    "# if only 1 sample, axes shape comes as (3,), normalize to 2D indexing\n",
    "if len(samples) == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for r, s in enumerate(samples):\n",
    "    # 1) Original\n",
    "    ax = axes[r, 0]\n",
    "    ax.imshow(s[\"img\"])\n",
    "    ax.set_title(\"Original\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # 2) Original + GT\n",
    "    ax = axes[r, 1]\n",
    "    ax.imshow(s[\"img\"])\n",
    "    ax.imshow(s[\"gt\"], cmap=cmap, vmin=0, vmax=vmax, alpha=OVERLAY_ALPHA, interpolation='nearest')\n",
    "    ax.set_title(\"Original + GT\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # 3) Original + Prediction\n",
    "    ax = axes[r, 2]\n",
    "    ax.imshow(s[\"img\"])\n",
    "    ax.imshow(s[\"pred\"], cmap=cmap, vmin=0, vmax=vmax, alpha=OVERLAY_ALPHA, interpolation='nearest')\n",
    "    ax.set_title(\"Original + Prediction\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standalone cell: Inference on ALL images in a folder -> [Original | Original + Prediction] ---\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Project imports (adjust if your package path differs)\n",
    "from mask2former import (\n",
    "    Mask2FormerFinetuner,\n",
    "    ID2LABEL,\n",
    "    LEARNING_RATE,\n",
    ")\n",
    "\n",
    "# ===================== USER CONFIG =====================\n",
    "CKPT_PATH   = \"/home/erik/Documents/Finetune-Mask2Former/outputs/lightning_logs_csv/version_4/checkpoints/epoch=6-step=6699.ckpt\"     # <-- change to your .ckpt\n",
    "IMAGES_DIR  = \"/home/erik/Documents/Finetune-Mask2Former/data/rs19/images/custom_test\"   # <-- folder with input images\n",
    "IMAGE_EXTS  = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "BATCH_SIZE  = 8            # batching for speed/memory\n",
    "OVERLAY_ALPHA = 0.5\n",
    "SEED_PALETTE  = 14\n",
    "\n",
    "# Display options:\n",
    "SHOW_MAX = -1              # -1 = show ALL; otherwise show only first N images in the notebook\n",
    "FIG_W    = 14              # width of the full figure\n",
    "ROW_H    = 3.2             # height per image row (figure height = ROW_H * num_rows)\n",
    "# =======================================================\n",
    "\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def list_images(folder: str, exts: set) -> List[Path]:\n",
    "    p = Path(folder)\n",
    "    files = [f for f in sorted(p.iterdir()) if f.suffix.lower() in exts and f.is_file()]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No images found in {folder} with extensions {exts}\")\n",
    "    return files\n",
    "\n",
    "def load_rgb(path: Path) -> Image.Image:\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def _make_palette(id2label, seed=SEED_PALETTE) -> Tuple[ListedColormap, int]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    max_id = max(id2label.keys()) if len(id2label) else 0\n",
    "    num_classes = max_id + 1\n",
    "    colors = rng.uniform(0, 1, size=(num_classes, 3))\n",
    "    if 0 in id2label:\n",
    "        colors[0] = np.array([0.0, 0.0, 0.0])  # background black for contrast\n",
    "    return ListedColormap(colors), max_id\n",
    "\n",
    "def overlay_image_with_mask(img_np: np.ndarray, mask: np.ndarray, cmap: ListedColormap, vmax: int, alpha: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns an RGBA overlay image drawn with matplotlib, then captured as numpy.\n",
    "    (We also handle visualization with plt directly below; this is for saving to disk.)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from io import BytesIO\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.imshow(img_np)\n",
    "    ax.imshow(mask, cmap=cmap, vmin=0, vmax=vmax, alpha=alpha, interpolation='nearest')\n",
    "    ax.axis(\"off\")\n",
    "    buf = BytesIO()\n",
    "    plt.tight_layout(pad=0)\n",
    "    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0, dpi=150)\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    out = Image.open(buf).convert(\"RGB\")\n",
    "    return np.array(out)\n",
    "\n",
    "def pil_to_numpy_uint8(img_pil: Image.Image) -> np.ndarray:\n",
    "    return np.array(img_pil, dtype=np.uint8)\n",
    "\n",
    "# ---------- load model ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Mask2FormerFinetuner.load_from_checkpoint(\n",
    "    CKPT_PATH,\n",
    "    id2label=ID2LABEL,\n",
    "    lr=LEARNING_RATE,\n",
    ").eval().to(device)\n",
    "\n",
    "processor = model.processor  # AutoImageProcessor from your finetuner\n",
    "cmap, vmax = _make_palette(ID2LABEL)\n",
    "\n",
    "# ---------- gather images ----------\n",
    "paths = list_images(IMAGES_DIR, IMAGE_EXTS)\n",
    "num_imgs = len(paths)\n",
    "print(f\"Found {num_imgs} images in: {IMAGES_DIR}\")\n",
    "\n",
    "# ---------- batched inference ----------\n",
    "all_originals = []   # list of np.uint8 HxWx3\n",
    "all_preds     = []   # list of int HxW\n",
    "\n",
    "with torch.no_grad():\n",
    "    # we will accumulate batches of PIL images and their target sizes\n",
    "    batch_imgs_pil = []\n",
    "    batch_target_sizes = []\n",
    "    batch_indices = []\n",
    "\n",
    "    def flush_batch():\n",
    "        if not batch_imgs_pil:\n",
    "            return\n",
    "        # preprocess as a batch\n",
    "        inputs = processor(images=batch_imgs_pil, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "        outputs = model.model(pixel_values=pixel_values)\n",
    "        pred_list = processor.post_process_semantic_segmentation(outputs, target_sizes=batch_target_sizes)\n",
    "\n",
    "        # store results\n",
    "        for idx_in_batch, pred in enumerate(pred_list):\n",
    "            pred_np = pred.detach().cpu().numpy()\n",
    "            img_np  = pil_to_numpy_uint8(batch_imgs_pil[idx_in_batch])\n",
    "            # Keep in arrays\n",
    "            all_originals.append(img_np)\n",
    "            all_preds.append(pred_np)\n",
    "\n",
    "        # clear batch\n",
    "        batch_imgs_pil.clear()\n",
    "        batch_target_sizes.clear()\n",
    "        batch_indices.clear()\n",
    "\n",
    "    for idx, p in enumerate(paths):\n",
    "        img_pil = load_rgb(p)\n",
    "        H, W = img_pil.size[1], img_pil.size[0]  # PIL: size = (W, H)\n",
    "        batch_imgs_pil.append(img_pil)\n",
    "        batch_target_sizes.append((H, W))\n",
    "        batch_indices.append(idx)\n",
    "\n",
    "        if len(batch_imgs_pil) == BATCH_SIZE:\n",
    "            flush_batch()\n",
    "\n",
    "    # leftover\n",
    "    flush_batch()\n",
    "\n",
    "# ---------- visualize ALL: 2 columns (Original | Original + Prediction) ----------\n",
    "to_show = len(all_originals) if SHOW_MAX < 0 else min(SHOW_MAX, len(all_originals))\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=to_show,\n",
    "    ncols=2,\n",
    "    figsize=(FIG_W, ROW_H * to_show),\n",
    ")\n",
    "\n",
    "# normalize axes to 2D even if single image\n",
    "if to_show == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for r in range(to_show):\n",
    "    img_np = all_originals[r]\n",
    "    pred   = all_preds[r]\n",
    "\n",
    "    # 1) Original\n",
    "    ax = axes[r, 0]\n",
    "    ax.imshow(img_np)\n",
    "    ax.set_title(f\"Original ({Path(paths[r]).name})\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # 2) Original + Prediction\n",
    "    ax = axes[r, 1]\n",
    "    ax.imshow(img_np)\n",
    "    ax.imshow(pred, cmap=cmap, vmin=0, vmax=vmax, alpha=OVERLAY_ALPHA, interpolation='nearest')\n",
    "    ax.set_title(\"Original + Prediction\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Project imports (adjust if your package path differs)\n",
    "from mask2former import (\n",
    "    Mask2FormerFinetuner,\n",
    "    ID2LABEL,\n",
    "    LEARNING_RATE,\n",
    ")\n",
    "\n",
    "# ===================== USER CONFIG =====================\n",
    "CKPT_PATH    = \"/home/erik/Documents/Finetune-Mask2Former/outputs/lightning_logs_csv/version_4/checkpoints/epoch=6-step=6699.ckpt\"\n",
    "IMAGES_DIR   = \"//home/erik/Desktop/test_imgs/imgs_26\"                                     \n",
    "OUTPUT_DIR   = \"/home/erik/Desktop/test_imgs/workspace_26/segmentation/mask2former\"                   \n",
    "IMAGE_EXTS   = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "BATCH_SIZE     = 64           # batching for speed/memory\n",
    "OVERLAY_ALPHA  = 0.5         # transparency of the segmentation overlay\n",
    "SEED_PALETTE   = 14          # to get consistent colors across runs\n",
    "\n",
    "# Optional: also save raw label mask as PNG (each pixel = class id)\n",
    "SAVE_RAW_MASKS = False\n",
    "RAW_MASKS_DIR  = \"/home/erik/Documents/Finetune-Mask2Former/outputs/masks\"\n",
    "# =======================================================\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def list_images(folder: str, exts: set) -> List[Path]:\n",
    "    p = Path(folder)\n",
    "    files = [f for f in sorted(p.iterdir()) if f.suffix.lower() in exts and f.is_file()]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No images found in {folder} with extensions {exts}\")\n",
    "    return files\n",
    "\n",
    "def load_rgb(path: Path) -> Image.Image:\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "def _make_palette(id2label, seed=SEED_PALETTE) -> Tuple[ListedColormap, int]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    max_id = max(id2label.keys()) if len(id2label) else 0\n",
    "    num_classes = max_id + 1\n",
    "    colors = rng.uniform(0, 1, size=(num_classes, 3))\n",
    "    if 0 in id2label:\n",
    "        colors[0] = np.array([0.0, 0.0, 0.0])  # background black\n",
    "    return ListedColormap(colors), max_id\n",
    "\n",
    "def overlay_image_with_mask(img_np: np.ndarray, mask: np.ndarray, cmap: ListedColormap, vmax: int, alpha: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Overlay segmentation mask directly onto the original RGB image (keeps original resolution).\n",
    "    \"\"\"\n",
    "    # colorize mask\n",
    "    mask_rgba = cmap(mask / vmax)  # normalized to [0,1]\n",
    "    mask_rgb = (mask_rgba[:, :, :3] * 255).astype(np.uint8)\n",
    "\n",
    "    # blend: img * (1-alpha) + mask * alpha (where mask != background)\n",
    "    overlay = img_np.copy()\n",
    "    non_bg = mask != 0\n",
    "    overlay[non_bg] = ((1 - alpha) * img_np[non_bg] + alpha * mask_rgb[non_bg]).astype(np.uint8)\n",
    "\n",
    "    return overlay\n",
    "\n",
    "\n",
    "# ---------- prep IO ----------\n",
    "paths = list_images(IMAGES_DIR, IMAGE_EXTS)\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "if SAVE_RAW_MASKS:\n",
    "    Path(RAW_MASKS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Found {len(paths)} images in: {IMAGES_DIR}\")\n",
    "print(f\"Saving overlays to: {OUTPUT_DIR}\")\n",
    "\n",
    "# ---------- load model ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Mask2FormerFinetuner.load_from_checkpoint(\n",
    "    CKPT_PATH,\n",
    "    id2label=ID2LABEL,\n",
    "    lr=LEARNING_RATE,\n",
    ").eval().to(device)\n",
    "\n",
    "processor = model.processor\n",
    "cmap, vmax = _make_palette(ID2LABEL)\n",
    "\n",
    "# ---------- batched inference and saving ----------\n",
    "with torch.no_grad():\n",
    "    batch_imgs_pil = []\n",
    "    batch_sizes = []\n",
    "    batch_paths = []\n",
    "\n",
    "    def flush_batch():\n",
    "        if not batch_imgs_pil:\n",
    "            return\n",
    "        inputs = processor(images=batch_imgs_pil, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "        outputs = model.model(pixel_values=pixel_values)\n",
    "        pred_list = processor.post_process_semantic_segmentation(outputs, target_sizes=batch_sizes)\n",
    "\n",
    "        for img_pil, mask_t, p in zip(batch_imgs_pil, pred_list, batch_paths):\n",
    "            mask_np = mask_t.detach().cpu().numpy().astype(np.int32)\n",
    "            img_np = np.array(img_pil, dtype=np.uint8)\n",
    "            overlay_np = overlay_image_with_mask(img_np, mask_np, cmap, vmax, OVERLAY_ALPHA)\n",
    "\n",
    "            # save overlay\n",
    "            out_path = Path(OUTPUT_DIR) / p.name\n",
    "            Image.fromarray(overlay_np).save(out_path)\n",
    "\n",
    "            if SAVE_RAW_MASKS:\n",
    "                mask_path = Path(RAW_MASKS_DIR) / (p.stem + \".png\")\n",
    "                Image.fromarray(mask_np.astype(np.uint16), mode=\"I;16\").save(mask_path)\n",
    "\n",
    "        batch_imgs_pil.clear()\n",
    "        batch_sizes.clear()\n",
    "        batch_paths.clear()\n",
    "\n",
    "    for p in tqdm(paths, desc=\"Processing images\"):\n",
    "        img_pil = load_rgb(p)\n",
    "        H, W = img_pil.size[1], img_pil.size[0]  # (H, W)\n",
    "        batch_imgs_pil.append(img_pil)\n",
    "        batch_sizes.append((H, W))\n",
    "        batch_paths.append(p)\n",
    "\n",
    "        if len(batch_imgs_pil) == BATCH_SIZE:\n",
    "            flush_batch()\n",
    "\n",
    "    flush_batch()\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mask2Former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
